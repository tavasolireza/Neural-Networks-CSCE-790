{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-26T19:55:07.603126Z",
     "start_time": "2023-10-26T19:55:05.683703Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T19:55:07.610814Z",
     "start_time": "2023-10-26T19:55:07.606416Z"
    }
   },
   "id": "ef0a184a55508e51"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T19:56:16.177748Z",
     "start_time": "2023-10-26T19:56:16.169972Z"
    }
   },
   "id": "696d89a55e2f6a23"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T19:56:36.615372Z",
     "start_time": "2023-10-26T19:56:36.607629Z"
    }
   },
   "id": "fd9816a953d36c71"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T19:56:47.589023Z",
     "start_time": "2023-10-26T19:56:47.570138Z"
    }
   },
   "id": "2ed88300cc81bd2a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T19:56:57.520605Z",
     "start_time": "2023-10-26T19:56:57.504219Z"
    }
   },
   "id": "6b4284849ff8b3ad"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T19:57:09.532251Z",
     "start_time": "2023-10-26T19:57:09.528251Z"
    }
   },
   "id": "9e6b96f17f849fba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing Sample Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1eaef2313ef45a8"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T19:58:41.194166Z",
     "start_time": "2023-10-26T19:58:40.989744Z"
    }
   },
   "id": "823b70db183f3dfd"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.680292129516602\n",
      "Epoch: 2, Loss: 8.54838752746582\n",
      "Epoch: 3, Loss: 8.472156524658203\n",
      "Epoch: 4, Loss: 8.419471740722656\n",
      "Epoch: 5, Loss: 8.368781089782715\n",
      "Epoch: 6, Loss: 8.296171188354492\n",
      "Epoch: 7, Loss: 8.214773178100586\n",
      "Epoch: 8, Loss: 8.13066291809082\n",
      "Epoch: 9, Loss: 8.052348136901855\n",
      "Epoch: 10, Loss: 7.975934982299805\n",
      "Epoch: 11, Loss: 7.89243221282959\n",
      "Epoch: 12, Loss: 7.808145523071289\n",
      "Epoch: 13, Loss: 7.72355842590332\n",
      "Epoch: 14, Loss: 7.635698318481445\n",
      "Epoch: 15, Loss: 7.552353382110596\n",
      "Epoch: 16, Loss: 7.472256183624268\n",
      "Epoch: 17, Loss: 7.3895263671875\n",
      "Epoch: 18, Loss: 7.309683322906494\n",
      "Epoch: 19, Loss: 7.226979732513428\n",
      "Epoch: 20, Loss: 7.160541534423828\n",
      "Epoch: 21, Loss: 7.084794044494629\n",
      "Epoch: 22, Loss: 6.992620944976807\n",
      "Epoch: 23, Loss: 6.923543453216553\n",
      "Epoch: 24, Loss: 6.8420491218566895\n",
      "Epoch: 25, Loss: 6.775001049041748\n",
      "Epoch: 26, Loss: 6.6926374435424805\n",
      "Epoch: 27, Loss: 6.6234025955200195\n",
      "Epoch: 28, Loss: 6.549030780792236\n",
      "Epoch: 29, Loss: 6.478429317474365\n",
      "Epoch: 30, Loss: 6.41352653503418\n",
      "Epoch: 31, Loss: 6.340864181518555\n",
      "Epoch: 32, Loss: 6.274647235870361\n",
      "Epoch: 33, Loss: 6.215487480163574\n",
      "Epoch: 34, Loss: 6.154218673706055\n",
      "Epoch: 35, Loss: 6.076119899749756\n",
      "Epoch: 36, Loss: 6.013127326965332\n",
      "Epoch: 37, Loss: 5.948099613189697\n",
      "Epoch: 38, Loss: 5.881587028503418\n",
      "Epoch: 39, Loss: 5.815688133239746\n",
      "Epoch: 40, Loss: 5.750468730926514\n",
      "Epoch: 41, Loss: 5.69267463684082\n",
      "Epoch: 42, Loss: 5.622295379638672\n",
      "Epoch: 43, Loss: 5.568739891052246\n",
      "Epoch: 44, Loss: 5.511695384979248\n",
      "Epoch: 45, Loss: 5.444919109344482\n",
      "Epoch: 46, Loss: 5.390073776245117\n",
      "Epoch: 47, Loss: 5.328338146209717\n",
      "Epoch: 48, Loss: 5.271149158477783\n",
      "Epoch: 49, Loss: 5.215280055999756\n",
      "Epoch: 50, Loss: 5.160281181335449\n",
      "Epoch: 51, Loss: 5.105403900146484\n",
      "Epoch: 52, Loss: 5.046786308288574\n",
      "Epoch: 53, Loss: 4.986392974853516\n",
      "Epoch: 54, Loss: 4.931668281555176\n",
      "Epoch: 55, Loss: 4.880331516265869\n",
      "Epoch: 56, Loss: 4.826385498046875\n",
      "Epoch: 57, Loss: 4.771966934204102\n",
      "Epoch: 58, Loss: 4.718392848968506\n",
      "Epoch: 59, Loss: 4.660433292388916\n",
      "Epoch: 60, Loss: 4.607440948486328\n",
      "Epoch: 61, Loss: 4.56290340423584\n",
      "Epoch: 62, Loss: 4.506378650665283\n",
      "Epoch: 63, Loss: 4.4571099281311035\n",
      "Epoch: 64, Loss: 4.404184341430664\n",
      "Epoch: 65, Loss: 4.357444763183594\n",
      "Epoch: 66, Loss: 4.301806926727295\n",
      "Epoch: 67, Loss: 4.256560802459717\n",
      "Epoch: 68, Loss: 4.209090709686279\n",
      "Epoch: 69, Loss: 4.1691107749938965\n",
      "Epoch: 70, Loss: 4.113376617431641\n",
      "Epoch: 71, Loss: 4.054807186126709\n",
      "Epoch: 72, Loss: 4.013730525970459\n",
      "Epoch: 73, Loss: 3.958299398422241\n",
      "Epoch: 74, Loss: 3.9151434898376465\n",
      "Epoch: 75, Loss: 3.8716137409210205\n",
      "Epoch: 76, Loss: 3.818293333053589\n",
      "Epoch: 77, Loss: 3.7695672512054443\n",
      "Epoch: 78, Loss: 3.723264455795288\n",
      "Epoch: 79, Loss: 3.6820290088653564\n",
      "Epoch: 80, Loss: 3.626826047897339\n",
      "Epoch: 81, Loss: 3.5851399898529053\n",
      "Epoch: 82, Loss: 3.530349016189575\n",
      "Epoch: 83, Loss: 3.4917728900909424\n",
      "Epoch: 84, Loss: 3.4414329528808594\n",
      "Epoch: 85, Loss: 3.4030191898345947\n",
      "Epoch: 86, Loss: 3.351452350616455\n",
      "Epoch: 87, Loss: 3.305464506149292\n",
      "Epoch: 88, Loss: 3.2572600841522217\n",
      "Epoch: 89, Loss: 3.219135284423828\n",
      "Epoch: 90, Loss: 3.1707866191864014\n",
      "Epoch: 91, Loss: 3.1247518062591553\n",
      "Epoch: 92, Loss: 3.086228847503662\n",
      "Epoch: 93, Loss: 3.0415494441986084\n",
      "Epoch: 94, Loss: 2.99995756149292\n",
      "Epoch: 95, Loss: 2.951261043548584\n",
      "Epoch: 96, Loss: 2.9173667430877686\n",
      "Epoch: 97, Loss: 2.8708415031433105\n",
      "Epoch: 98, Loss: 2.8260087966918945\n",
      "Epoch: 99, Loss: 2.784557342529297\n",
      "Epoch: 100, Loss: 2.7410058975219727\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T20:04:00.241006Z",
     "start_time": "2023-10-26T19:58:49.538830Z"
    }
   },
   "id": "74afb6c1ee8b118c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Define Basic Building Blocks\n",
    "\n",
    "#### 1.1 Multi-Head Attention:\n",
    "- **Purpose**: To compute attention scores between each pair of positions in a sequence. This allows the model to focus on different aspects of the input sequence.\n",
    "- **Components**:\n",
    "  - Multiple attention heads that process the input in parallel and capture different relationships.\n",
    "  - Linear transformation layers that transform the input.\n",
    "- **How it works**:\n",
    "  1. Initialize the module with input parameters and define the linear transformation layers.\n",
    "  2. Calculate attention scores.\n",
    "  3. Reshape the input tensor to create multiple heads.\n",
    "  4. Combine the attention outputs from all heads.\n",
    "\n",
    "#### 1.2 Position-wise Feed-Forward Networks:\n",
    "- **Purpose**: To transform the output of the attention layers.\n",
    "- **Components**:\n",
    "  - Two linear transformation layers.\n",
    "  - A ReLU activation function.\n",
    "- **How it works**:\n",
    "  1. Initialize the class with the transformation layers and the activation function.\n",
    "  2. During the forward pass, apply the transformations and activation function sequentially.\n",
    "\n",
    "#### 1.3 Positional Encoding:\n",
    "- **Purpose**: To provide the model with information about the position of tokens in the sequence since the Transformer does not have any inherent notion of order.\n",
    "- **Components**:\n",
    "  - Sine and cosine functions used to generate position-specific values.\n",
    "- **How it works**:\n",
    "  1. Initialize the class and create a tensor to store positional encoding values.\n",
    "  2. Calculate sine and cosine values for different positions.\n",
    "  3. During the forward pass, add the positional encoding values to the input tensor.\n",
    "\n",
    "### 2. Building Encoder and Decoder Layers\n",
    "\n",
    "#### 2.1 Encoder Layer:\n",
    "- **Components**:\n",
    "  - Multi-Head Attention layer.\n",
    "  - Position-wise Feed-Forward layer.\n",
    "  - Two Layer Normalization layers.\n",
    "- **How it works**:\n",
    "  1. Initialize the class with its components.\n",
    "  2. During the forward pass, apply self-attention, then add the attention output to the input tensor and normalize it.\n",
    "  3. Compute the position-wise feed-forward output, combine it with the normalized self-attention output, and normalize again.\n",
    "\n",
    "#### 2.2 Decoder Layer:\n",
    "- **Components**:\n",
    "  - Two Multi-Head Attention layers (for masked self-attention and cross-attention).\n",
    "  - Position-wise Feed-Forward layer.\n",
    "  - Three Layer Normalization layers.\n",
    "- **How it works**:\n",
    "  1. Initialize the class with its components.\n",
    "  2. During the forward pass:\n",
    "     - Calculate the masked self-attention output, add it to the input, apply dropout, and normalize.\n",
    "     - Compute cross-attention between decoder and encoder outputs, normalize, and combine with masked self-attention.\n",
    "     - Calculate position-wise feed-forward output, combine with previous outputs, apply dropout, and normalize.\n",
    "\n",
    "### 3. Build the Complete Transformer Model\n",
    "\n",
    "#### 3.1 Transformer Model:\n",
    "- **Components**:\n",
    "  - Embedding layers for source and target sequences.\n",
    "  - Positional Encoding module.\n",
    "  - Stacked Encoder and Decoder layers.\n",
    "  - Linear layer for projecting decoder output.\n",
    "- **How it works**:\n",
    "  1. Initialize the class and its components.\n",
    "  2. Define the `generate_mask` method to create masks for source and target sequences.\n",
    "  3. During the forward pass:\n",
    "     - Generate masks for source and target sequences.\n",
    "     - Compute embeddings and apply positional encoding and dropout.\n",
    "     - Process the source sequence through the encoder layers.\n",
    "     - Process the target sequence through the decoder layers, using encoder outputs and masks.\n",
    "     - Apply the linear projection layer to the decoder output to obtain the final logits.\n",
    "\n",
    "The Transformer model processes input sequences and produces output sequences by combining the functionalities of its components, ensuring attention is paid to relevant parts of the input and capturing complex relationships between input and output."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4984623e0a7a84c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "88f005ab13dc2d6c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
